{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO6kg898DqRf"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import XLMRobertaModel, XLMRobertaTokenizerFast\n",
        "import json\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants\n",
        "MAX_SEQ_LENGTH = 512\n",
        "BATCH_SIZE = 8\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUM_LABELS_SPAN = 3\n",
        "NUM_LABELS_POLARITY = 4\n",
        "NUM_LABELS_INTENSITY = 3\n",
        "SPAN_EMBEDDING_DIM = 768\n",
        "RELATION_EMBEDDING_DIM = 256\n",
        "ADAPTER_SIZE = 128\n",
        "\n",
        "# Label mappings\n",
        "polarity_map = {'Positive': 0, 'Negative': 1, 'Neutral': 2, 'None': 3}\n",
        "intensity_map = {'Strong': 0, 'Average': 1, 'Weak': 2}\n",
        "polarity_reverse_map = {v: k for k, v in polarity_map.items()}\n",
        "intensity_reverse_map = {v: k for k, v in intensity_map.items()}\n",
        "\n",
        "# Neural network modules\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads=8, head_dim=96):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.query = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.key = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.value = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.output_projection = nn.Linear(num_heads * head_dim, input_dim)\n",
        "        self.layer_norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "        output = self.output_projection(context)\n",
        "        return self.layer_norm(output + x)\n",
        "\n",
        "class SpanDetector(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_labels=NUM_LABELS_SPAN):\n",
        "        super(SpanDetector, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.activation(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class CrossSpanAttention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=RELATION_EMBEDDING_DIM):\n",
        "        super(CrossSpanAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(input_dim, num_heads=4, batch_first=True)\n",
        "        self.projection = nn.Linear(input_dim, output_dim)\n",
        "        self.layer_norm = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, spans, span_masks=None):\n",
        "        key_padding_mask = ~span_masks if span_masks is not None else None\n",
        "        context, _ = self.attention(spans, spans, spans, key_padding_mask=key_padding_mask)\n",
        "        output = self.projection(context)\n",
        "        return self.layer_norm(output)\n",
        "\n",
        "class RelationClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_labels=2):\n",
        "        super(RelationClassifier, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim * 2, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, span_pairs):\n",
        "        x = self.hidden(span_pairs)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class PolarityClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_labels=NUM_LABELS_POLARITY):\n",
        "        super(PolarityClassifier, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class IntensityClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_labels=NUM_LABELS_INTENSITY):\n",
        "        super(IntensityClassifier, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class LanguageAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=ADAPTER_SIZE):\n",
        "        super(LanguageAdapter, self).__init__()\n",
        "        self.down_project = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.up_project = nn.Linear(bottleneck_dim, input_dim)\n",
        "        self.layer_norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.down_project(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.up_project(x)\n",
        "        return self.layer_norm(x + residual)\n",
        "\n",
        "class StructuredSentimentModel(nn.Module):\n",
        "    def __init__(self, pretrained_model_name=\"xlm-roberta-base\", use_adapters=False, num_languages=8):\n",
        "        super(StructuredSentimentModel, self).__init__()\n",
        "        self.encoder = XLMRobertaModel.from_pretrained(pretrained_model_name)\n",
        "        self.hidden_size = self.encoder.config.hidden_size\n",
        "        self.span_attention = SelfAttentionLayer(self.hidden_size)\n",
        "        self.holder_detector = SpanDetector(self.hidden_size)\n",
        "        self.target_detector = SpanDetector(self.hidden_size)\n",
        "        self.expression_detector = SpanDetector(self.hidden_size)\n",
        "        self.cross_span_attention = CrossSpanAttention(self.hidden_size)\n",
        "        self.relation_classifier = RelationClassifier(RELATION_EMBEDDING_DIM)\n",
        "        self.polarity_classifier = PolarityClassifier(RELATION_EMBEDDING_DIM)\n",
        "        self.intensity_classifier = IntensityClassifier(RELATION_EMBEDDING_DIM)\n",
        "        self.use_adapters = use_adapters\n",
        "        if use_adapters:\n",
        "            self.language_adapters = nn.ModuleList([LanguageAdapter(self.hidden_size) for _ in range(num_languages)])\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        modules = [self.span_attention, self.holder_detector, self.target_detector,\n",
        "                   self.expression_detector, self.cross_span_attention,\n",
        "                   self.relation_classifier, self.polarity_classifier, self.intensity_classifier]\n",
        "        for module in modules:\n",
        "            for name, param in module.named_parameters():\n",
        "                if 'weight' in name and len(param.shape) >= 2:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "    def extract_spans(self, span_logits, attention_mask):\n",
        "        batch_size = span_logits.size(0)\n",
        "        span_preds = torch.argmax(torch.softmax(span_logits, dim=-1), dim=-1)\n",
        "        all_spans = []\n",
        "        for i in range(batch_size):\n",
        "            mask = attention_mask[i].bool()\n",
        "            preds = span_preds[i][mask]\n",
        "            spans = []\n",
        "            start_idx = None\n",
        "            for j, label in enumerate(preds):\n",
        "                if label == 1:  # B\n",
        "                    if start_idx is not None:\n",
        "                        spans.append((start_idx, j - 1))\n",
        "                    start_idx = j\n",
        "                elif label == 0:  # O\n",
        "                    if start_idx is not None:\n",
        "                        spans.append((start_idx, j - 1))\n",
        "                        start_idx = None\n",
        "                elif label == 2:  # I\n",
        "                    if start_idx is None:\n",
        "                        start_idx = j\n",
        "            if start_idx is not None:\n",
        "                spans.append((start_idx, len(preds) - 1))\n",
        "            all_spans.append(spans)\n",
        "        return all_spans\n",
        "\n",
        "    def get_span_embeddings(self, hidden_states, spans, attention_mask):\n",
        "        batch_size = hidden_states.size(0)\n",
        "        max_spans = max([len(s) for s in spans], default=0)\n",
        "        if max_spans == 0:\n",
        "            return torch.zeros((batch_size, 0, self.hidden_size), device=hidden_states.device), torch.zeros((batch_size, 0), dtype=torch.bool, device=hidden_states.device)\n",
        "        span_embeddings = torch.zeros((batch_size, max_spans, self.hidden_size), device=hidden_states.device)\n",
        "        span_masks = torch.zeros((batch_size, max_spans), dtype=torch.bool, device=hidden_states.device)\n",
        "        for i in range(batch_size):\n",
        "            for j, (start, end) in enumerate(spans[i]):\n",
        "                if j < max_spans:\n",
        "                    span_embeddings[i, j] = hidden_states[i, start:end+1].mean(dim=0)\n",
        "                    span_masks[i, j] = True\n",
        "        return span_embeddings, span_masks\n",
        "\n",
        "    def _combine_spans(self, holder_emb, holder_mask, target_emb, target_mask, expr_emb, expr_mask):\n",
        "        batch_size = holder_emb.size(0)\n",
        "        max_spans = holder_emb.size(1) + target_emb.size(1) + expr_emb.size(1)\n",
        "        if max_spans == 0:\n",
        "            return torch.zeros((batch_size, 0, self.hidden_size), device=holder_emb.device), torch.zeros((batch_size, 0), dtype=torch.bool, device=holder_emb.device)\n",
        "        combined_emb = torch.zeros((batch_size, max_spans, self.hidden_size), device=holder_emb.device)\n",
        "        combined_mask = torch.zeros((batch_size, max_spans), dtype=torch.bool, device=holder_emb.device)\n",
        "        holder_size = holder_emb.size(1)\n",
        "        target_size = target_emb.size(1)\n",
        "        expr_size = expr_emb.size(1)\n",
        "        combined_emb[:, :holder_size] = holder_emb\n",
        "        combined_emb[:, holder_size:holder_size+target_size] = target_emb\n",
        "        combined_emb[:, holder_size+target_size:] = expr_emb\n",
        "        combined_mask[:, :holder_size] = holder_mask\n",
        "        combined_mask[:, holder_size:holder_size+target_size] = target_mask\n",
        "        combined_mask[:, holder_size+target_size:] = expr_mask\n",
        "        return combined_emb, combined_mask\n",
        "\n",
        "    def _create_span_pairs(self, span_embeddings, holder_mask, target_mask, expr_mask):\n",
        "        batch_size = span_embeddings.size(0)\n",
        "        holder_size = holder_mask.size(1)\n",
        "        target_size = target_mask.size(1)\n",
        "        expr_size = expr_mask.size(1)\n",
        "        total_holders = holder_mask.sum(dim=1)\n",
        "        total_targets = target_mask.sum(dim=1)\n",
        "        total_expressions = expr_mask.sum(dim=1)\n",
        "        max_pairs = torch.max(total_holders * total_expressions + total_targets * total_expressions)\n",
        "        if max_pairs == 0:\n",
        "            return None, None\n",
        "        pair_embeddings = torch.zeros((batch_size, max_pairs, RELATION_EMBEDDING_DIM * 2), device=span_embeddings.device)\n",
        "        pair_indices = torch.zeros((batch_size, max_pairs, 2), dtype=torch.long, device=span_embeddings.device)\n",
        "        offset = holder_size + target_size\n",
        "        for i in range(batch_size):\n",
        "            pair_idx = 0\n",
        "            for h_idx in range(holder_size):\n",
        "                if not holder_mask[i, h_idx]:\n",
        "                    continue\n",
        "                for e_idx in range(expr_size):\n",
        "                    if not expr_mask[i, e_idx] or pair_idx >= max_pairs:\n",
        "                        continue\n",
        "                    pair_embeddings[i, pair_idx] = torch.cat([span_embeddings[i, h_idx], span_embeddings[i, offset + e_idx]])\n",
        "                    pair_indices[i, pair_idx] = torch.tensor([h_idx, offset + e_idx], device=span_embeddings.device)\n",
        "                    pair_idx += 1\n",
        "            for t_idx in range(target_size):\n",
        "                if not target_mask[i, t_idx]:\n",
        "                    continue\n",
        "                for e_idx in range(expr_size):\n",
        "                    if not expr_mask[i, e_idx] or pair_idx >= max_pairs:\n",
        "                        continue\n",
        "                    pair_embeddings[i, pair_idx] = torch.cat([span_embeddings[i, holder_size + t_idx], span_embeddings[i, offset + e_idx]])\n",
        "                    pair_indices[i, pair_idx] = torch.tensor([holder_size + t_idx, offset + e_idx], device=span_embeddings.device)\n",
        "                    pair_idx += 1\n",
        "        return pair_embeddings, pair_indices\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, language_id=None, labels=None):\n",
        "        batch_size = input_ids.size(0)\n",
        "        encoder_outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = encoder_outputs.last_hidden_state\n",
        "        if self.use_adapters and language_id is not None:\n",
        "            adapted_states = torch.zeros_like(hidden_states)\n",
        "            for i in range(batch_size):\n",
        "                adapted_states[i] = self.language_adapters[language_id[i].item()](hidden_states[i])\n",
        "            hidden_states = adapted_states\n",
        "        span_aware_states = self.span_attention(hidden_states, attention_mask)\n",
        "        holder_logits = self.holder_detector(span_aware_states)\n",
        "        target_logits = self.target_detector(span_aware_states)\n",
        "        expression_logits = self.expression_detector(span_aware_states)\n",
        "        if labels is not None:\n",
        "            pass\n",
        "        else:\n",
        "            holder_spans = self.extract_spans(holder_logits, attention_mask)\n",
        "            target_spans = self.extract_spans(target_logits, attention_mask)\n",
        "            expression_spans = self.extract_spans(expression_logits, attention_mask)\n",
        "            holder_embeddings, holder_masks = self.get_span_embeddings(span_aware_states, holder_spans, attention_mask)\n",
        "            target_embeddings, target_masks = self.get_span_embeddings(span_aware_states, target_spans, attention_mask)\n",
        "            expression_embeddings, expression_masks = self.get_span_embeddings(span_aware_states, expression_spans, attention_mask)\n",
        "            all_span_embeddings, all_span_masks = self._combine_spans(holder_embeddings, holder_masks, target_embeddings, target_masks, expression_embeddings, expression_masks)\n",
        "            relation_aware_embeddings = self.cross_span_attention(all_span_embeddings, all_span_masks)\n",
        "            relation_pairs, pair_indices = self._create_span_pairs(relation_aware_embeddings, holder_masks, target_masks, expression_masks)\n",
        "            relation_logits = None\n",
        "            polarity_logits = None\n",
        "            intensity_logits = None\n",
        "            if relation_pairs is not None:\n",
        "                relation_logits = self.relation_classifier(relation_pairs)\n",
        "                expression_relation_aware = relation_aware_embeddings[:, holder_embeddings.size(1) + target_embeddings.size(1):, :]\n",
        "                polarity_logits = self.polarity_classifier(expression_relation_aware)\n",
        "                intensity_logits = self.intensity_classifier(expression_relation_aware)\n",
        "            return {\n",
        "                'holder_logits': holder_logits,\n",
        "                'target_logits': target_logits,\n",
        "                'expression_logits': expression_logits,\n",
        "                'relation_logits': relation_logits,\n",
        "                'polarity_logits': polarity_logits,\n",
        "                'intensity_logits': intensity_logits,\n",
        "                'holder_spans': holder_spans,\n",
        "                'target_spans': target_spans,\n",
        "                'expression_spans': expression_spans,\n",
        "                'pair_indices': pair_indices\n",
        "            }\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self, model_path, pretrained_model=\"xlm-roberta-base\", device=None):\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "        self.tokenizer = XLMRobertaTokenizerFast.from_pretrained(pretrained_model)\n",
        "        self.model = StructuredSentimentModel(pretrained_model_name=pretrained_model)\n",
        "        checkpoint = torch.load(model_path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        logger.info(f\"Model loaded from {model_path}\")\n",
        "        self.polarity_map = {0: \"Positive\", 1: \"Negative\", 2: \"Neutral\", 3: \"None\"}\n",
        "        self.intensity_map = {0: \"Strong\", 1: \"Average\", 2: \"Weak\"}\n",
        "\n",
        "    def _get_actual_text_spans(self, text, tokens, spans):\n",
        "        text_spans = []\n",
        "        offset_mapping = tokens.offset_mapping[0].tolist()\n",
        "        for start_token, end_token in spans:\n",
        "            if start_token >= len(offset_mapping) or end_token >= len(offset_mapping):\n",
        "                continue\n",
        "            start_char = offset_mapping[start_token][0]\n",
        "            end_char = offset_mapping[end_token][1]\n",
        "            if start_char < end_char and end_char <= len(text):\n",
        "                span_text = text[start_char:end_char]\n",
        "                text_spans.append((start_char, end_char, span_text))\n",
        "        return text_spans\n",
        "\n",
        "    def analyze(self, text):\n",
        "        tokens = self.tokenizer(\n",
        "            text,\n",
        "            max_length=MAX_SEQ_LENGTH,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_offsets_mapping=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = tokens['input_ids'].to(self.device)\n",
        "        attention_mask = tokens['attention_mask'].to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        holder_spans = outputs['holder_spans'][0]\n",
        "        target_spans = outputs['target_spans'][0]\n",
        "        expression_spans = outputs['expression_spans'][0]\n",
        "        holder_text_spans = self._get_actual_text_spans(text, tokens, holder_spans)\n",
        "        target_text_spans = self._get_actual_text_spans(text, tokens, target_spans)\n",
        "        expression_text_spans = self._get_actual_text_spans(text, tokens, expression_spans)\n",
        "        sentiment_opinions = []\n",
        "        if outputs['polarity_logits'] is not None and outputs['intensity_logits'] is not None:\n",
        "            polarity_preds = torch.argmax(outputs['polarity_logits'], dim=-1)\n",
        "            intensity_preds = torch.argmax(outputs['intensity_logits'], dim=-1)\n",
        "            num_expressions = min(len(expression_text_spans), polarity_preds.size(1))\n",
        "            for i in range(num_expressions):\n",
        "                polarity_idx = polarity_preds[0, i].item()\n",
        "                intensity_idx = intensity_preds[0, i].item()\n",
        "                opinion = {\n",
        "                    \"expression\": expression_text_spans[i][2],\n",
        "                    \"expression_span\": f\"{expression_text_spans[i][0]}:{expression_text_spans[i][1]}\",\n",
        "                    \"polarity\": self.polarity_map[polarity_idx],\n",
        "                    \"intensity\": self.intensity_map[intensity_idx],\n",
        "                }\n",
        "                if outputs['pair_indices'] is not None:\n",
        "                    for pair_idx in range(outputs['pair_indices'].size(1)):\n",
        "                        idx1, idx2 = outputs['pair_indices'][0, pair_idx]\n",
        "                        expr_offset = len(holder_spans) + len(target_spans)\n",
        "                        if idx2 == expr_offset + i:\n",
        "                            if idx1 < len(holder_spans):\n",
        "                                holder_idx = idx1.item()\n",
        "                                if holder_idx < len(holder_text_spans):\n",
        "                                    opinion[\"holder\"] = holder_text_spans[holder_idx][2]\n",
        "                                    opinion[\"holder_span\"] = f\"{holder_text_spans[holder_idx][0]}:{holder_text_spans[holder_idx][1]}\"\n",
        "                            else:\n",
        "                                target_idx = idx1.item() - len(holder_spans)\n",
        "                                if target_idx < len(target_text_spans):\n",
        "                                    opinion[\"target\"] = target_text_spans[target_idx][2]\n",
        "                                    opinion[\"target_span\"] = f\"{target_text_spans[target_idx][0]}:{target_text_spans[target_idx][1]}\"\n",
        "                if \"holder\" not in opinion:\n",
        "                    opinion[\"holder\"] = \"\"\n",
        "                    opinion[\"holder_span\"] = \"0:0\"\n",
        "                if \"target\" not in opinion:\n",
        "                    opinion[\"target\"] = \"\"\n",
        "                    opinion[\"target_span\"] = \"0:0\"\n",
        "                sentiment_opinions.append(opinion)\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"holders\": [span[2] for span in holder_text_spans],\n",
        "            \"targets\": [span[2] for span in target_text_spans],\n",
        "            \"expressions\": [span[2] for span in expression_text_spans],\n",
        "            \"opinions\": sentiment_opinions\n",
        "        }\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=MAX_SEQ_LENGTH):\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.examples = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        for entry in data:\n",
        "            processed = self.process_example(entry)\n",
        "            if processed:\n",
        "                self.examples.append(processed)\n",
        "\n",
        "    def process_example(self, entry):\n",
        "        text = entry.get('text', '')\n",
        "        sent_id = entry.get('sent_id', '')\n",
        "        opinions = entry.get('opinions', [])\n",
        "        if not text:\n",
        "            return None\n",
        "        tokenized = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_offsets_mapping=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        offset_mapping = tokenized['offset_mapping'][0]\n",
        "        input_ids = tokenized['input_ids'][0]\n",
        "        attention_mask = tokenized['attention_mask'][0]\n",
        "        holder_labels = torch.zeros(self.max_length, dtype=torch.long)\n",
        "        target_labels = torch.zeros(self.max_length, dtype=torch.long)\n",
        "        expression_labels = torch.zeros(self.max_length, dtype=torch.long)\n",
        "        opinion_data = []\n",
        "        for opinion in opinions:\n",
        "            holder_span = self._extract_span(opinion, 'Source', offset_mapping)\n",
        "            target_span = self._extract_span(opinion, 'Target', offset_mapping)\n",
        "            expression_span = self._extract_span(opinion, 'Polar_expression', offset_mapping)\n",
        "            polarity = polarity_map.get(opinion.get('Polarity', 'None'), 3)\n",
        "            intensity = intensity_map.get(opinion.get('Intensity', 'Average'), 1)\n",
        "            if all(span is not None for span in [holder_span, target_span, expression_span]):\n",
        "                opinion_data.append({\n",
        "                    'holder_span': holder_span,\n",
        "                    'target_span': target_span,\n",
        "                    'expression_span': expression_span,\n",
        "                    'polarity': polarity,\n",
        "                    'intensity': intensity,\n",
        "                })\n",
        "                self._mark_span(holder_labels, holder_span[0], holder_span[1])\n",
        "                self._mark_span(target_labels, target_span[0], target_span[1])\n",
        "                self._mark_span(expression_labels, expression_span[0], expression_span[1])\n",
        "        return {\n",
        "            'sent_id': sent_id,\n",
        "            'text': text,\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'holder_labels': holder_labels,\n",
        "            'target_labels': target_labels,\n",
        "            'expression_labels': expression_labels,\n",
        "            'opinion_data': opinion_data,\n",
        "            'offset_mapping': offset_mapping\n",
        "        }\n",
        "\n",
        "    def _extract_span(self, opinion, key, offset_mapping):\n",
        "        span_data = opinion.get(key, [[], []])[1]\n",
        "        if not span_data:\n",
        "            return (0, 0)\n",
        "        try:\n",
        "            start, end = map(int, span_data[0].split(':'))\n",
        "            start_token = end_token = 0\n",
        "            for idx, (ts, te) in enumerate(offset_mapping):\n",
        "                if ts == 0 and te == 0:\n",
        "                    continue\n",
        "                if ts <= start < te:\n",
        "                    start_token = idx\n",
        "                if ts < end <= te:\n",
        "                    end_token = idx\n",
        "            return (start_token, end_token)\n",
        "        except Exception:\n",
        "            return (0, 0)\n",
        "\n",
        "    def _mark_span(self, labels, start_idx, end_idx):\n",
        "        try:\n",
        "            labels[start_idx] = 1\n",
        "            if end_idx > start_idx:\n",
        "                labels[start_idx+1:end_idx+1] = 2\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Span marking error {start_idx}:{end_idx}: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "def custom_collate(batch):\n",
        "    collated = {\n",
        "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "        'holder_labels': torch.stack([item['holder_labels'] for item in batch]),\n",
        "        'target_labels': torch.stack([item['target_labels'] for item in batch]),\n",
        "        'expression_labels': torch.stack([item['expression_labels'] for item in batch]),\n",
        "        'sent_id': [item['sent_id'] for item in batch],\n",
        "        'text': [item['text'] for item in batch],\n",
        "        'opinion_data': [item['opinion_data'] for item in batch],\n",
        "        'offset_mapping': torch.stack([item['offset_mapping'] for item in batch])\n",
        "    }\n",
        "    return collated\n",
        "\n",
        "def token_span_to_char_span(token_span, offset_mapping, text):\n",
        "    if token_span == (0, 0):\n",
        "        return [[], []]\n",
        "    start, end = token_span\n",
        "    char_start = offset_mapping[start, 0].item()\n",
        "    char_end = offset_mapping[end, 1].item()\n",
        "    span_text = text[char_start:char_end]\n",
        "    offset_str = f\"{char_start}:{char_end}\"\n",
        "    return [[span_text], [offset_str]]\n",
        "\n",
        "def opinions_match(pred_op, gt_op):\n",
        "    pred_holder, pred_target, pred_expr, pred_pol, pred_int = pred_op\n",
        "    gt_holder, gt_target, gt_expr, gt_pol, gt_int = gt_op\n",
        "    pred_int_adj = 'Standard' if pred_int == 'Average' else pred_int\n",
        "    gt_int_adj = 'Standard' if gt_int == 'Average' else gt_int\n",
        "    return (pred_holder == gt_holder and\n",
        "            pred_target == gt_target and\n",
        "            pred_expr == gt_expr and\n",
        "            pred_pol == gt_pol and\n",
        "            pred_int_adj == gt_int_adj)\n",
        "\n",
        "def evaluate_predictions(all_predicted, all_ground_truth):\n",
        "    tp = fp = fn = 0\n",
        "    for sent_pred, sent_gt in zip(all_predicted, all_ground_truth):\n",
        "        matched_preds = set()\n",
        "        for gt_op in sent_gt:\n",
        "            for i, pred_op in enumerate(sent_pred):\n",
        "                if i not in matched_preds and opinions_match(pred_op, gt_op):\n",
        "                    tp += 1\n",
        "                    matched_preds.add(i)\n",
        "                    break\n",
        "            else:\n",
        "                fn += 1\n",
        "        fp += len(sent_pred) - len(matched_preds)\n",
        "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Upload test.json\n",
        "print(\"Upload your test.json file:\")\n",
        "uploaded = files.upload()\n",
        "test_file = list(uploaded.keys())[0]\n",
        "with open(test_file, 'wb') as f:\n",
        "    f.write(uploaded[test_file])\n",
        "\n",
        "# Initialize SentimentAnalyzer\n",
        "model_path = \"/content/drive/MyDrive/NLP-Project/opener_en_best_model_f1_0.7344.pt\"\n",
        "pretrained_model = \"xlm-roberta-base\"\n",
        "analyzer = SentimentAnalyzer(model_path, pretrained_model, DEVICE)\n",
        "\n",
        "# Data setup\n",
        "tokenizer = analyzer.tokenizer\n",
        "test_dataset = SentimentDataset(test_file, tokenizer)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                            collate_fn=custom_collate)\n",
        "\n",
        "# Prediction loop\n",
        "all_predicted = []\n",
        "all_ground_truth = []\n",
        "all_sentences = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Predicting\"):\n",
        "        for i in range(len(batch['sent_id'])):\n",
        "            sent_id = batch['sent_id'][i]\n",
        "            text = batch['text'][i]\n",
        "            offset_mapping = batch['offset_mapping'][i]\n",
        "            analysis = analyzer.analyze(text)\n",
        "            predicted_opinions = []\n",
        "            for opinion in analysis['opinions']:\n",
        "                holder_span = (0, 0)\n",
        "                target_span = (0, 0)\n",
        "                expression_span = (0, 0)\n",
        "                if opinion['holder_span'] != \"0:0\":\n",
        "                    start_char, end_char = map(int, opinion['holder_span'].split(':'))\n",
        "                    for idx, (ts, te) in enumerate(offset_mapping):\n",
        "                        if ts <= start_char < te:\n",
        "                            holder_span = (idx, holder_span[1])\n",
        "                        if ts < end_char <= te:\n",
        "                            holder_span = (holder_span[0], idx)\n",
        "                if opinion['target_span'] != \"0:0\":\n",
        "                    start_char, end_char = map(int, opinion['target_span'].split(':'))\n",
        "                    for idx, (ts, te) in enumerate(offset_mapping):\n",
        "                        if ts <= start_char < te:\n",
        "                            target_span = (idx, target_span[1])\n",
        "                        if ts < end_char <= te:\n",
        "                            target_span = (target_span[0], idx)\n",
        "                if opinion['expression_span'] != \"0:0\":\n",
        "                    start_char, end_char = map(int, opinion['expression_span'].split(':'))\n",
        "                    for idx, (ts, te) in enumerate(offset_mapping):\n",
        "                        if ts <= start_char < te:\n",
        "                            expression_span = (idx, expression_span[1])\n",
        "                        if ts < end_char <= te:\n",
        "                            expression_span = (expression_span[0], idx)\n",
        "                predicted_opinions.append((\n",
        "                    holder_span,\n",
        "                    target_span,\n",
        "                    expression_span,\n",
        "                    opinion['polarity'],\n",
        "                    opinion['intensity']\n",
        "                ))\n",
        "            if len(all_predicted) < 3:\n",
        "                print(f\"\\nSentence: {text}\")\n",
        "                print(f\"Predicted: {predicted_opinions}\")\n",
        "                print(f\"Ground Truth: {batch['opinion_data'][i]}\")\n",
        "            opinions_list = []\n",
        "            for opinion in analysis['opinions']:\n",
        "                source = [[opinion['holder']], [opinion['holder_span']]] if opinion['holder'] else [[], []]\n",
        "                target = [[opinion['target']], [opinion['target_span']]] if opinion['target'] else [[], []]\n",
        "                polar_expr = [[opinion['expression']], [opinion['expression_span']]] if opinion['expression'] else [[], []]\n",
        "                opinion_dict = {\n",
        "                    \"Source\": source,\n",
        "                    \"Target\": target,\n",
        "                    \"Polar_expression\": polar_expr,\n",
        "                    \"Polarity\": opinion['polarity'],\n",
        "                    \"Intensity\": opinion['intensity']\n",
        "                }\n",
        "                opinions_list.append(opinion_dict)\n",
        "            sentence_dict = {\n",
        "                \"sent_id\": sent_id,\n",
        "                \"text\": text,\n",
        "                \"opinions\": opinions_list\n",
        "            }\n",
        "            all_sentences.append(sentence_dict)\n",
        "            gt_opinions = [\n",
        "                (\n",
        "                    op['holder_span'],\n",
        "                    op['target_span'],\n",
        "                    op['expression_span'],\n",
        "                    polarity_reverse_map[op['polarity']],\n",
        "                    intensity_reverse_map[op['intensity']]\n",
        "                ) for op in batch['opinion_data'][i]\n",
        "            ]\n",
        "            all_predicted.append(predicted_opinions)\n",
        "            all_ground_truth.append(gt_opinions)\n",
        "\n",
        "# Save output\n",
        "output_file = 'output.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_sentences, f, indent=2)\n",
        "logger.info(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "# Evaluate\n",
        "precision, recall, f1 = evaluate_predictions(all_predicted, all_ground_truth)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Check empty predictions\n",
        "empty_count = sum(1 for pred in all_predicted if not pred)\n",
        "print(f\"Empty predictions: {empty_count}/{len(all_predicted)}\")\n",
        "\n",
        "# Download output\n",
        "files.download(output_file)"
      ]
    }
  ]
}