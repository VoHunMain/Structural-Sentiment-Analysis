{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfd9H8OvDnN4"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# import os\n",
        "# import json\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.optim import XLMRobertaModel, XLMRobertaTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import logging\n",
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW  # AdamW comes from torch.optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import XLMRobertaModel, XLMRobertaTokenizerFast, get_linear_schedule_with_warmup\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "DEBUG = False\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define constants\n",
        "MAX_SEQ_LENGTH = 512\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_STEPS = 0\n",
        "ADAPTER_SIZE = 128\n",
        "NUM_LABELS_SPAN = 3  # 0:O, 1:B, 2:I\n",
        "NUM_LABELS_POLARITY = 4  # Positive, Negative, Neutral, None\n",
        "NUM_LABELS_INTENSITY = 3  # Strong, Average, Weak\n",
        "NUM_LABELS_RELATION = 2  # Related, Not Related\n",
        "SPAN_EMBEDDING_DIM = 768\n",
        "RELATION_EMBEDDING_DIM = 256\n",
        "\n",
        "# Custom collate function for DataLoader\n",
        "def custom_collate(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    collated_batch = {}\n",
        "    fixed_keys = ['input_ids', 'attention_mask', 'holder_labels', 'target_labels', 'expression_labels']\n",
        "    for key in fixed_keys:\n",
        "        if key in batch[0]:\n",
        "            collated_batch[key] = torch.stack([item[key] for item in batch])\n",
        "    for key in batch[0].keys():\n",
        "        if key not in fixed_keys:\n",
        "            collated_batch[key] = [item[key] for item in batch]\n",
        "    return collated_batch\n",
        "\n",
        "# Dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=MAX_SEQ_LENGTH):\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.examples = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            first_line = f.readline().strip()\n",
        "            f.seek(0)\n",
        "            if first_line.startswith('['):\n",
        "                data = json.load(f)\n",
        "            else:\n",
        "                data = [json.loads(line) for line in f if line.strip()]\n",
        "        logger.info(f\"Loaded {len(data)} examples from {file_path}\")\n",
        "        for entry in data:\n",
        "            if isinstance(entry, dict):\n",
        "                processed = self.process_example(entry)\n",
        "                if processed:\n",
        "                    self.examples.append(processed)\n",
        "\n",
        "    def process_example(self, entry):\n",
        "        try:\n",
        "            text = entry.get('text', '')\n",
        "            sent_id = entry.get('sent_id', '')\n",
        "            opinions = entry.get('opinions', [])\n",
        "            if not text:\n",
        "                logger.warning(f\"Empty text for entry with sent_id {sent_id}\")\n",
        "                return None\n",
        "\n",
        "            tokenized = self.tokenizer(\n",
        "                text,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_offsets_mapping=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            offset_mapping = tokenized['offset_mapping'][0]\n",
        "            input_ids = tokenized['input_ids'][0]\n",
        "            attention_mask = tokenized['attention_mask'][0]\n",
        "\n",
        "            holder_labels = torch.zeros(self.max_length, dtype=torch.long)\n",
        "            target_labels = torch.zeros(self.max_length, dtype=torch.long)\n",
        "            expression_labels = torch.zeros(self.max_length, dtype=torch.long)\n",
        "\n",
        "            opinion_data = []\n",
        "            for opinion in opinions:\n",
        "                if isinstance(opinion, dict):\n",
        "                    holder_span = self._extract_span_safely(opinion, 'Source', offset_mapping)\n",
        "                    target_span = self._extract_span_safely(opinion, 'Target', offset_mapping)\n",
        "                    expression_span = self._extract_span_safely(opinion, 'Polar_expression', offset_mapping)\n",
        "                    polarity = self._get_polarity_label(opinion.get('Polarity', 'None'))\n",
        "                    intensity = self._get_intensity_label(opinion.get('Intensity', 'Average'))\n",
        "\n",
        "                    if all(span is not None for span in [holder_span, target_span, expression_span]):\n",
        "                        opinion_data.append({\n",
        "                            'holder_span': holder_span,\n",
        "                            'target_span': target_span,\n",
        "                            'expression_span': expression_span,\n",
        "                            'polarity': polarity,\n",
        "                            'intensity': intensity,\n",
        "                        })\n",
        "                        self._mark_span(holder_labels, holder_span[0], holder_span[1])\n",
        "                        self._mark_span(target_labels, target_span[0], target_span[1])\n",
        "                        self._mark_span(expression_labels, expression_span[0], expression_span[1])\n",
        "\n",
        "            return {\n",
        "                'sent_id': sent_id,\n",
        "                'text': text,\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'holder_labels': holder_labels,\n",
        "                'target_labels': target_labels,\n",
        "                'expression_labels': expression_labels,\n",
        "                'opinion_data': opinion_data,\n",
        "                'num_opinions': len(opinion_data)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing example: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_span_safely(self, opinion, key, offset_mapping):\n",
        "        span_data = opinion.get(key)\n",
        "        if not span_data:\n",
        "            return (0, 0)\n",
        "        try:\n",
        "            span_str = None\n",
        "            if isinstance(span_data, list):\n",
        "                if len(span_data) > 1:\n",
        "                    candidate = span_data[1]\n",
        "                    span_str = candidate[0] if isinstance(candidate, list) and candidate else candidate\n",
        "                else:\n",
        "                    candidate = span_data[0]\n",
        "                    span_str = candidate[1] if isinstance(candidate, list) and len(candidate) > 1 else candidate\n",
        "            elif isinstance(span_data, str):\n",
        "                span_str = span_data\n",
        "            else:\n",
        "                return (0, 0)\n",
        "            if not span_str or \":\" not in span_str:\n",
        "                return (0, 0)\n",
        "            return self._get_token_span(span_str, offset_mapping)\n",
        "        except Exception:\n",
        "            return (0, 0)\n",
        "\n",
        "    def _get_token_span(self, span_str, offset_mapping):\n",
        "        try:\n",
        "            start, end = map(int, span_str.split(':'))\n",
        "            start_token = end_token = 1\n",
        "            for idx, (ts, te) in enumerate(offset_mapping):\n",
        "                ts, te = int(ts), int(te)\n",
        "                if ts == 0 and te == 0:\n",
        "                    continue\n",
        "                if ts <= start < te:\n",
        "                    start_token = idx\n",
        "                if ts < end:\n",
        "                    end_token = idx\n",
        "            return (start_token, end_token)\n",
        "        except Exception:\n",
        "            return (0, 0)\n",
        "\n",
        "    def _mark_span(self, labels, start_idx, end_idx):\n",
        "        try:\n",
        "            labels[start_idx] = 1  # B\n",
        "            if end_idx > start_idx:\n",
        "                labels[start_idx+1:end_idx+1] = 2  # I\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error marking span {start_idx}:{end_idx}: {e}\")\n",
        "\n",
        "    def _get_polarity_label(self, polarity):\n",
        "        polarity_map = {'Positive': 0, 'Negative': 1, 'Neutral': 2, 'None': 3}\n",
        "        return polarity_map.get(polarity, 3)\n",
        "\n",
        "    def _get_intensity_label(self, intensity):\n",
        "        intensity_map = {'Strong': 0, 'Average': 1, 'Weak': 2}\n",
        "        return intensity_map.get(intensity, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "# Neural network modules\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads=8, head_dim=96):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.query = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.key = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.value = nn.Linear(input_dim, num_heads * head_dim)\n",
        "        self.output_projection = nn.Linear(num_heads * head_dim, input_dim)\n",
        "        self.layer_norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "        output = self.output_projection(context)\n",
        "        return self.layer_norm(output + x)\n",
        "\n",
        "class SpanDetector(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_labels=NUM_LABELS_SPAN):\n",
        "        super(SpanDetector, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.activation(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class CrossSpanAttention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=RELATION_EMBEDDING_DIM):\n",
        "        super(CrossSpanAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(input_dim, num_heads=4, batch_first=True)\n",
        "        self.projection = nn.Linear(input_dim, output_dim)\n",
        "        self.layer_norm = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, spans, span_masks=None):\n",
        "        key_padding_mask = ~span_masks if span_masks is not None else None\n",
        "        context, _ = self.attention(spans, spans, spans, key_padding_mask=key_padding_mask)\n",
        "        output = self.projection(context)\n",
        "        return self.layer_norm(output)\n",
        "\n",
        "class RelationClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_labels=NUM_LABELS_RELATION):\n",
        "        super(RelationClassifier, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim * 2, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, span_pairs):\n",
        "        x = self.hidden(span_pairs)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class PolarityClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_labels=NUM_LABELS_POLARITY):\n",
        "        super(PolarityClassifier, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class IntensityClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_labels=NUM_LABELS_INTENSITY):\n",
        "        super(IntensityClassifier, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class LanguageAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=ADAPTER_SIZE):\n",
        "        super(LanguageAdapter, self).__init__()\n",
        "        self.down_project = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.up_project = nn.Linear(bottleneck_dim, input_dim)\n",
        "        self.layer_norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.down_project(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.up_project(x)\n",
        "        return self.layer_norm(x + residual)\n",
        "\n",
        "# Main model class\n",
        "class StructuredSentimentModel(nn.Module):\n",
        "    def __init__(self, pretrained_model_name=\"xlm-roberta-base\", use_adapters=False, num_languages=8):\n",
        "        super(StructuredSentimentModel, self).__init__()\n",
        "        self.encoder = XLMRobertaModel.from_pretrained(pretrained_model_name)\n",
        "        self.hidden_size = self.encoder.config.hidden_size\n",
        "        self.span_attention = SelfAttentionLayer(self.hidden_size)\n",
        "        self.holder_detector = SpanDetector(self.hidden_size)\n",
        "        self.target_detector = SpanDetector(self.hidden_size)\n",
        "        self.expression_detector = SpanDetector(self.hidden_size)\n",
        "        self.cross_span_attention = CrossSpanAttention(self.hidden_size)\n",
        "        self.relation_classifier = RelationClassifier(RELATION_EMBEDDING_DIM)\n",
        "        self.polarity_classifier = PolarityClassifier(RELATION_EMBEDDING_DIM)\n",
        "        self.intensity_classifier = IntensityClassifier(RELATION_EMBEDDING_DIM)\n",
        "        self.use_adapters = use_adapters\n",
        "        if use_adapters:\n",
        "            self.language_adapters = nn.ModuleList([LanguageAdapter(self.hidden_size) for _ in range(num_languages)])\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        modules = [self.span_attention, self.holder_detector, self.target_detector,\n",
        "                   self.expression_detector, self.cross_span_attention,\n",
        "                   self.relation_classifier, self.polarity_classifier, self.intensity_classifier]\n",
        "        for module in modules:\n",
        "            for name, param in module.named_parameters():\n",
        "                if 'weight' in name and len(param.shape) >= 2:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "    def extract_spans(self, span_logits, attention_mask):\n",
        "        batch_size = span_logits.size(0)\n",
        "        span_preds = torch.argmax(torch.softmax(span_logits, dim=-1), dim=-1)\n",
        "        all_spans = []\n",
        "        for i in range(batch_size):\n",
        "            mask = attention_mask[i].bool()\n",
        "            preds = span_preds[i][mask]\n",
        "            spans = []\n",
        "            start_idx = None\n",
        "            for j, label in enumerate(preds):\n",
        "                if label == 1:  # B\n",
        "                    if start_idx is not None:\n",
        "                        spans.append((start_idx, j - 1))\n",
        "                    start_idx = j\n",
        "                elif label == 0:  # O\n",
        "                    if start_idx is not None:\n",
        "                        spans.append((start_idx, j - 1))\n",
        "                        start_idx = None\n",
        "            if start_idx is not None:\n",
        "                spans.append((start_idx, len(preds) - 1))\n",
        "            all_spans.append(spans)\n",
        "        return all_spans\n",
        "\n",
        "    def get_span_embeddings(self, hidden_states, spans, attention_mask):\n",
        "        batch_size = hidden_states.size(0)\n",
        "        max_spans = max([len(s) for s in spans], default=0)\n",
        "        if max_spans == 0:\n",
        "            return torch.zeros((batch_size, 0, self.hidden_size), device=hidden_states.device), torch.zeros((batch_size, 0), dtype=torch.bool, device=hidden_states.device)\n",
        "        span_embeddings = torch.zeros((batch_size, max_spans, self.hidden_size), device=hidden_states.device)\n",
        "        span_masks = torch.zeros((batch_size, max_spans), dtype=torch.bool, device=hidden_states.device)\n",
        "        for i in range(batch_size):\n",
        "            for j, (start, end) in enumerate(spans[i]):\n",
        "                if j < max_spans:\n",
        "                    span_embeddings[i, j] = hidden_states[i, start:end+1].mean(dim=0)\n",
        "                    span_masks[i, j] = True\n",
        "        return span_embeddings, span_masks\n",
        "\n",
        "    def _combine_spans(self, holder_emb, holder_mask, target_emb, target_mask, expr_emb, expr_mask):\n",
        "        batch_size = holder_emb.size(0)\n",
        "        max_spans = holder_emb.size(1) + target_emb.size(1) + expr_emb.size(1)\n",
        "        if max_spans == 0:\n",
        "            return torch.zeros((batch_size, 0, self.hidden_size), device=holder_emb.device), torch.zeros((batch_size, 0), dtype=torch.bool, device=holder_emb.device)\n",
        "        combined_emb = torch.zeros((batch_size, max_spans, self.hidden_size), device=holder_emb.device)\n",
        "        combined_mask = torch.zeros((batch_size, max_spans), dtype=torch.bool, device=holder_emb.device)\n",
        "        holder_size = holder_emb.size(1)\n",
        "        target_size = target_emb.size(1)\n",
        "        expr_size = expr_emb.size(1)\n",
        "        combined_emb[:, :holder_size] = holder_emb\n",
        "        combined_emb[:, holder_size:holder_size+target_size] = target_emb\n",
        "        combined_emb[:, holder_size+target_size:] = expr_emb\n",
        "        combined_mask[:, :holder_size] = holder_mask\n",
        "        combined_mask[:, holder_size:holder_size+target_size] = target_mask\n",
        "        combined_mask[:, holder_size+target_size:] = expr_mask\n",
        "        return combined_emb, combined_mask\n",
        "\n",
        "    def _create_span_pairs(self, span_embeddings, holder_mask, target_mask, expr_mask):\n",
        "        batch_size = span_embeddings.size(0)\n",
        "        holder_size = holder_mask.size(1)\n",
        "        target_size = target_mask.size(1)\n",
        "        expr_size = expr_mask.size(1)\n",
        "        total_holders = holder_mask.sum(dim=1)\n",
        "        total_targets = target_mask.sum(dim=1)\n",
        "        total_expressions = expr_mask.sum(dim=1)\n",
        "        max_pairs = torch.max(total_holders * total_expressions + total_targets * total_expressions)\n",
        "        if max_pairs == 0:\n",
        "            return None, None\n",
        "        pair_embeddings = torch.zeros((batch_size, max_pairs, RELATION_EMBEDDING_DIM * 2), device=span_embeddings.device)\n",
        "        pair_indices = torch.zeros((batch_size, max_pairs, 2), dtype=torch.long, device=span_embeddings.device)\n",
        "        offset = holder_size + target_size\n",
        "        for i in range(batch_size):\n",
        "            pair_idx = 0\n",
        "            for h_idx in range(holder_size):\n",
        "                if not holder_mask[i, h_idx]:\n",
        "                    continue\n",
        "                for e_idx in range(expr_size):\n",
        "                    if not expr_mask[i, e_idx] or pair_idx >= max_pairs:\n",
        "                        continue\n",
        "                    pair_embeddings[i, pair_idx] = torch.cat([span_embeddings[i, h_idx], span_embeddings[i, offset + e_idx]])\n",
        "                    pair_indices[i, pair_idx] = torch.tensor([h_idx, offset + e_idx], device=span_embeddings.device)\n",
        "                    pair_idx += 1\n",
        "            for t_idx in range(target_size):\n",
        "                if not target_mask[i, t_idx]:\n",
        "                    continue\n",
        "                for e_idx in range(expr_size):\n",
        "                    if not expr_mask[i, e_idx] or pair_idx >= max_pairs:\n",
        "                        continue\n",
        "                    pair_embeddings[i, pair_idx] = torch.cat([span_embeddings[i, holder_size + t_idx], span_embeddings[i, offset + e_idx]])\n",
        "                    pair_indices[i, pair_idx] = torch.tensor([holder_size + t_idx, offset + e_idx], device=span_embeddings.device)\n",
        "                    pair_idx += 1\n",
        "        return pair_embeddings, pair_indices\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, language_id=None, labels=None):\n",
        "        batch_size = input_ids.size(0)\n",
        "        encoder_outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = encoder_outputs.last_hidden_state\n",
        "\n",
        "        if self.use_adapters and language_id is not None:\n",
        "            adapted_states = torch.zeros_like(hidden_states)\n",
        "            for i in range(batch_size):\n",
        "                adapted_states[i] = self.language_adapters[language_id[i].item()](hidden_states[i])\n",
        "            hidden_states = adapted_states\n",
        "\n",
        "        span_aware_states = self.span_attention(hidden_states, attention_mask)\n",
        "        holder_logits = self.holder_detector(span_aware_states)\n",
        "        target_logits = self.target_detector(span_aware_states)\n",
        "        expression_logits = self.expression_detector(span_aware_states)\n",
        "\n",
        "        if labels is not None:\n",
        "            holder_labels, target_labels, expression_labels, gold_opinions = labels\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            holder_loss = loss_fct(holder_logits.view(-1, NUM_LABELS_SPAN)[attention_mask.view(-1) == 1], holder_labels.view(-1)[attention_mask.view(-1) == 1])\n",
        "            target_loss = loss_fct(target_logits.view(-1, NUM_LABELS_SPAN)[attention_mask.view(-1) == 1], target_labels.view(-1)[attention_mask.view(-1) == 1])\n",
        "            expression_loss = loss_fct(expression_logits.view(-1, NUM_LABELS_SPAN)[attention_mask.view(-1) == 1], expression_labels.view(-1)[attention_mask.view(-1) == 1])\n",
        "            span_loss = holder_loss + target_loss + expression_loss\n",
        "\n",
        "            # Compute polarity and intensity losses using gold opinions\n",
        "            all_expr_emb = []\n",
        "            all_polarity_labels = []\n",
        "            all_intensity_labels = []\n",
        "            for i in range(batch_size):\n",
        "                opinions = gold_opinions[i]\n",
        "                if not opinions:\n",
        "                    continue\n",
        "                gold_holder_spans = [op['holder_span'] for op in opinions]\n",
        "                gold_target_spans = [op['target_span'] for op in opinions]\n",
        "                gold_expr_spans = [op['expression_span'] for op in opinions]\n",
        "                holder_emb, _ = self.get_span_embeddings(span_aware_states[i:i+1], [gold_holder_spans], attention_mask[i:i+1])\n",
        "                target_emb, _ = self.get_span_embeddings(span_aware_states[i:i+1], [gold_target_spans], attention_mask[i:i+1])\n",
        "                expr_emb, _ = self.get_span_embeddings(span_aware_states[i:i+1], [gold_expr_spans], attention_mask[i:i+1])\n",
        "                all_emb, all_mask = self._combine_spans(\n",
        "                    holder_emb, torch.ones_like(holder_emb[..., 0], dtype=torch.bool),\n",
        "                    target_emb, torch.ones_like(target_emb[..., 0], dtype=torch.bool),\n",
        "                    expr_emb, torch.ones_like(expr_emb[..., 0], dtype=torch.bool)\n",
        "                )\n",
        "                relation_aware_emb = self.cross_span_attention(all_emb, all_mask)\n",
        "                expr_relation_aware = relation_aware_emb[:, -len(gold_expr_spans):, :]\n",
        "                all_expr_emb.append(expr_relation_aware[0])\n",
        "                all_polarity_labels.extend([op['polarity'] for op in opinions])\n",
        "                all_intensity_labels.extend([op['intensity'] for op in opinions])\n",
        "\n",
        "            if all_expr_emb:\n",
        "                all_expr_emb = torch.cat(all_expr_emb, dim=0)\n",
        "                polarity_logits = self.polarity_classifier(all_expr_emb)\n",
        "                intensity_logits = self.intensity_classifier(all_expr_emb)\n",
        "                gold_polarity = torch.tensor(all_polarity_labels, dtype=torch.long, device=hidden_states.device)\n",
        "                gold_intensity = torch.tensor(all_intensity_labels, dtype=torch.long, device=hidden_states.device)\n",
        "                polarity_loss = loss_fct(polarity_logits, gold_polarity)\n",
        "                intensity_loss = loss_fct(intensity_logits, gold_intensity)\n",
        "                total_loss = span_loss + polarity_loss + intensity_loss\n",
        "            else:\n",
        "                total_loss = span_loss\n",
        "\n",
        "            return {\n",
        "                'loss': total_loss,\n",
        "                'holder_logits': holder_logits,\n",
        "                'target_logits': target_logits,\n",
        "                'expression_logits': expression_logits\n",
        "            }\n",
        "\n",
        "        # Inference mode\n",
        "        holder_spans = self.extract_spans(holder_logits, attention_mask)\n",
        "        target_spans = self.extract_spans(target_logits, attention_mask)\n",
        "        expression_spans = self.extract_spans(expression_logits, attention_mask)\n",
        "        holder_embeddings, holder_masks = self.get_span_embeddings(span_aware_states, holder_spans, attention_mask)\n",
        "        target_embeddings, target_masks = self.get_span_embeddings(span_aware_states, target_spans, attention_mask)\n",
        "        expression_embeddings, expression_masks = self.get_span_embeddings(span_aware_states, expression_spans, attention_mask)\n",
        "        all_span_embeddings, all_span_masks = self._combine_spans(holder_embeddings, holder_masks, target_embeddings, target_masks, expression_embeddings, expression_masks)\n",
        "        relation_aware_embeddings = self.cross_span_attention(all_span_embeddings, all_span_masks)\n",
        "        relation_pairs, pair_indices = self._create_span_pairs(relation_aware_embeddings, holder_masks, target_masks, expression_masks)\n",
        "\n",
        "        relation_logits = None\n",
        "        polarity_logits = None\n",
        "        intensity_logits = None\n",
        "        if relation_pairs is not None:\n",
        "            relation_logits = self.relation_classifier(relation_pairs)\n",
        "            expression_relation_aware = relation_aware_embeddings[:, holder_embeddings.size(1) + target_embeddings.size(1):, :]\n",
        "            polarity_logits = self.polarity_classifier(expression_relation_aware)\n",
        "            intensity_logits = self.intensity_classifier(expression_relation_aware)\n",
        "\n",
        "        return {\n",
        "            'holder_logits': holder_logits,\n",
        "            'target_logits': target_logits,\n",
        "            'expression_logits': expression_logits,\n",
        "            'relation_logits': relation_logits,\n",
        "            'polarity_logits': polarity_logits,\n",
        "            'intensity_logits': intensity_logits,\n",
        "            'holder_spans': holder_spans,\n",
        "            'target_spans': target_spans,\n",
        "            'expression_spans': expression_spans,\n",
        "            'pair_indices': pair_indices\n",
        "        }\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_dataloader, dev_dataloader, optimizer, scheduler, device, num_epochs, output_dir):\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        logger.info(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_steps = 0\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            holder_labels = batch['holder_labels'].to(device)\n",
        "            target_labels = batch['target_labels'].to(device)\n",
        "            expression_labels = batch['expression_labels'].to(device)\n",
        "            gold_opinions = batch['opinion_data']  # List of lists\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=(holder_labels, target_labels, expression_labels, gold_opinions)\n",
        "            )\n",
        "            loss = outputs['loss']\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "            train_steps += 1\n",
        "        avg_train_loss = train_loss / train_steps if train_steps > 0 else 0\n",
        "        logger.info(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        eval_results = evaluate_model(model, dev_dataloader, device)\n",
        "        avg_f1 = sum(m['f1'] for m in eval_results.values()) / len(eval_results)\n",
        "        logger.info(f\"Epoch {epoch+1} - Evaluation F1: {avg_f1:.4f}\")\n",
        "        if avg_f1 > best_f1:\n",
        "            best_f1 = avg_f1\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_f1': best_f1\n",
        "            }, os.path.join(output_dir, f\"best_model_f1_{best_f1:.4f}.pt\"))\n",
        "    return model\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, device, output_json=\"evaluation_results.json\"):\n",
        "    model.eval()\n",
        "    holder_preds, holder_true = [], []\n",
        "    target_preds, target_true = [], []\n",
        "    expression_preds, expression_true = [], []\n",
        "    polarity_preds, polarity_true = [], []\n",
        "    intensity_preds, intensity_true = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            holder_labels = batch['holder_labels'].to(device)\n",
        "            target_labels = batch['target_labels'].to(device)\n",
        "            expression_labels = batch['expression_labels'].to(device)\n",
        "            gold_opinions = batch['opinion_data']  # List of lists of opinion dicts\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Token-level span predictions\n",
        "            for i in range(input_ids.size(0)):\n",
        "                mask = attention_mask[i].bool()\n",
        "                seq_len = mask.sum().item()\n",
        "                holder_preds.extend(torch.argmax(outputs['holder_logits'][i, :seq_len], dim=-1).cpu().tolist())\n",
        "                holder_true.extend(holder_labels[i, :seq_len].cpu().tolist())\n",
        "                target_preds.extend(torch.argmax(outputs['target_logits'][i, :seq_len], dim=-1).cpu().tolist())\n",
        "                target_true.extend(target_labels[i, :seq_len].cpu().tolist())\n",
        "                expression_preds.extend(torch.argmax(outputs['expression_logits'][i, :seq_len], dim=-1).cpu().tolist())\n",
        "                expression_true.extend(expression_labels[i, :seq_len].cpu().tolist())\n",
        "\n",
        "            # Polarity and intensity predictions\n",
        "            if outputs['polarity_logits'] is not None and outputs['intensity_logits'] is not None:\n",
        "                polarity_logits = outputs['polarity_logits']  # Shape: (batch_size, num_expressions, 4)\n",
        "                intensity_logits = outputs['intensity_logits']  # Shape: (batch_size, num_expressions, 3)\n",
        "                batch_polarity_preds = torch.argmax(polarity_logits, dim=-1).cpu().tolist()  # Shape: (batch_size, num_expressions)\n",
        "                batch_intensity_preds = torch.argmax(intensity_logits, dim=-1).cpu().tolist()  # Shape: (batch_size, num_expressions)\n",
        "\n",
        "                for i in range(len(gold_opinions)):\n",
        "                    opinions = gold_opinions[i]\n",
        "                    num_pred_expressions = len(batch_polarity_preds[i]) if i < len(batch_polarity_preds) else 0\n",
        "                    num_gold_opinions = len(opinions)\n",
        "\n",
        "                    # Align predictions with ground truth\n",
        "                    for j in range(max(num_pred_expressions, num_gold_opinions)):\n",
        "                        if j < num_pred_expressions:\n",
        "                            pred_pol = batch_polarity_preds[i][j]\n",
        "                            pred_int = batch_intensity_preds[i][j]\n",
        "                        else:\n",
        "                            pred_pol = 3  # None for polarity if no prediction\n",
        "                            pred_int = 1  # Average for intensity if no prediction\n",
        "\n",
        "                        if j < num_gold_opinions:\n",
        "                            true_pol = opinions[j]['polarity']\n",
        "                            true_int = opinions[j]['intensity']\n",
        "                        else:\n",
        "                            true_pol = 3  # None for polarity if no ground truth\n",
        "                            true_int = 1  # Average for intensity if no ground truth\n",
        "\n",
        "                        polarity_preds.append(pred_pol)\n",
        "                        polarity_true.append(true_pol)\n",
        "                        intensity_preds.append(pred_int)\n",
        "                        intensity_true.append(true_int)\n",
        "\n",
        "    # Compute token-level span metrics\n",
        "    holder_prf = precision_recall_fscore_support(holder_true, holder_preds, average='macro', zero_division=0)\n",
        "    target_prf = precision_recall_fscore_support(target_true, target_preds, average='macro', zero_division=0)\n",
        "    expression_prf = precision_recall_fscore_support(expression_true, expression_preds, average='macro', zero_division=0)\n",
        "\n",
        "    # Compute polarity and intensity metrics\n",
        "    polarity_prf = precision_recall_fscore_support(polarity_true, polarity_preds, average='macro', zero_division=0)\n",
        "    intensity_prf = precision_recall_fscore_support(intensity_true, intensity_preds, average='macro', zero_division=0)\n",
        "\n",
        "    # Combine results\n",
        "    results = {\n",
        "        'holder': {'precision': holder_prf[0], 'recall': holder_prf[1], 'f1': holder_prf[2]},\n",
        "        'target': {'precision': target_prf[0], 'recall': target_prf[1], 'f1': target_prf[2]},\n",
        "        'expression': {'precision': expression_prf[0], 'recall': expression_prf[1], 'f1': expression_prf[2]},\n",
        "        'polarity': {'precision': polarity_prf[0], 'recall': polarity_prf[1], 'f1': polarity_prf[2]},\n",
        "        'intensity': {'precision': intensity_prf[0], 'recall': intensity_prf[1], 'f1': intensity_prf[2]}\n",
        "    }\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({'metrics': results}, f, indent=2)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    input_path = '/content'\n",
        "    output_dir = '/content'\n",
        "    data_dir = os.path.join(input_path, 'sentiment-data')\n",
        "    language = 'opener_en'\n",
        "    pretrained_model = 'xlm-roberta-base'\n",
        "    use_adapters = False\n",
        "    num_epochs = NUM_EPOCHS\n",
        "    batch_size = BATCH_SIZE\n",
        "    learning_rate = LEARNING_RATE\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    logger.addHandler(logging.FileHandler(os.path.join(output_dir, 'training.log')))\n",
        "\n",
        "    train_file = os.path.join(data_dir, language, \"/content/train.json\")\n",
        "    dev_file = os.path.join(data_dir, language, \"/content/dev.json\")\n",
        "    test_file = os.path.join(data_dir, language, \"/content/test.json\")\n",
        "\n",
        "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(pretrained_model)\n",
        "    train_dataset = SentimentDataset(train_file, tokenizer)\n",
        "    dev_dataset = SentimentDataset(dev_file, tokenizer)\n",
        "    test_dataset = SentimentDataset(test_file, tokenizer)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate, num_workers=2, pin_memory=True)\n",
        "    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate, num_workers=2, pin_memory=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate, num_workers=2, pin_memory=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = StructuredSentimentModel(pretrained_model_name=pretrained_model, use_adapters=use_adapters).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=len(train_dataloader) * num_epochs)\n",
        "\n",
        "    model = train_model(model, train_dataloader, dev_dataloader, optimizer, scheduler, device, num_epochs, output_dir)\n",
        "    test_results = evaluate_model(model, test_dataloader, device)\n",
        "    logger.info(\"Test Results:\")\n",
        "    for span_type, metrics in test_results.items():\n",
        "        logger.info(f\"{span_type}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'test_results': test_results\n",
        "    }, os.path.join(output_dir, \"final_model.pt\"))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}